2025-07-19 10:10:06,908	INFO worker.py:1879 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
[36m(WorkerDict pid=536753)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=536753)[0m [rank0]:[W719 10:11:03.590752973 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[36m(WorkerDict pid=536753)[0m Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
[36m(WorkerDict pid=537149)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=537149)[0m [rank2]:[W719 10:11:03.600019108 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=536753)[0m Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  3.34it/s]
[36m(WorkerDict pid=536753)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.18it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.19it/s]
[36m(WorkerDict pid=536753)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=537149)[0m Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=537149)[0m Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  3.12it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=537149)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.17it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.17it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=536753)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[36m(WorkerDict pid=537150)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=536753)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.28s/it]
[36m(WorkerDict pid=536753)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.28s/it]
[36m(WorkerDict pid=536753)[0m 
[36m(WorkerDict pid=537148)[0m /home/hk-project-p0022560/hgf_teb8892/envs/miniconda3/envs/pure/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=537148)[0m   warnings.warn(
[36m(main_task pid=530241)[0m wandb: Currently logged in as: ccwangzitong (ccwangzitong-technical-university-of-munich) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(WorkerDict pid=537149)[0m /home/hk-project-p0022560/hgf_teb8892/envs/miniconda3/envs/pure/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=537149)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(main_task pid=530241)[0m wandb: Tracking run with wandb version 0.19.11
[36m(main_task pid=530241)[0m wandb: Run data is saved locally in /hkfs/home/project/hk-project-p0022560/hgf_teb8892/projects/Phi_PURE/wandb/run-20250719_101235-2r1uxxon
[36m(main_task pid=530241)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=530241)[0m wandb: Syncing run Qwen2p5-1.5B-instruct_prompts-64_n-4
[36m(main_task pid=530241)[0m wandb: â­ï¸ View project at https://wandb.ai/ccwangzitong-technical-university-of-munich/pure_verl
[36m(main_task pid=530241)[0m wandb: ðŸš€ View run at https://wandb.ai/ccwangzitong-technical-university-of-munich/pure_verl/runs/2r1uxxon
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m WARNING:2025-07-19 10:55:06,144:WARNING: Error in configuration: macro '\frac' failed its substitution!
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m WARNING:2025-07-19 12:30:54,433:WARNING: Error in configuration: macro '\frac' failed its substitution!
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'float' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'float' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'float' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'float' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'set' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m WARNING:2025-07-19 14:02:54,056:WARNING: Error in configuration: macro '\frac' failed its substitution!
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'float' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'float' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'float' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m <string>:1: SyntaxWarning: 'float' object is not callable; perhaps you missed a comma?
[36m(main_task pid=530241)[0m WARNING:2025-07-19 14:59:26,864:WARNING: Error in configuration: macro '\frac' failed its substitution!
[36m(main_task pid=530241)[0m WARNING:2025-07-19 14:59:26,864:WARNING: Error in configuration: macro '\frac' failed its substitution!
[36m(main_task pid=530241)[0m WARNING:2025-07-19 15:42:58,165:WARNING: Error in configuration: macro '\frac' failed its substitution!
